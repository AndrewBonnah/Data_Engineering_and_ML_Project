{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b2c545b",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Stock ML Scraping, Cleaning, and Analysis\n",
    "</h1>\n",
    "\n",
    "<p>\n",
    "This project is intended to be a body of work showing my ability to code and analyze data.\n",
    "\n",
    "No AI tools were used.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a329a46e",
   "metadata": {},
   "source": [
    "The architecture I am planning is:\n",
    "\n",
    "yfinance (scrape most active tickers) -> Google Finance (get basic financial info) -> 12api (get stock price for a certain period)\n",
    "\n",
    "Put these together into data to be analyzed with ML to try and define a strategy to \"beat\" the market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d68d8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrew/Desktop/Code/code1/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NVDA', 'SNAP', 'NIO', 'PLUG', 'KVUE', 'INTC', 'ONDS', 'PLTR', 'CADE', 'GOOGL', 'F', 'SOFI', 'MARA', 'AMZN', 'GRAB', 'HIMS', 'OWL', 'TSLA', 'BMNR', 'PFE', 'CFLT', 'CCC', 'BBD', 'OPEN', 'AMD', 'MSFT', 'NU', 'SMCI', 'HOOD', 'ABEV', 'IREN', 'MSTR', 'NVO', 'ACHR', 'AAPL', 'VALE', 'NFLX', 'CPNG', 'GOOG', 'DNN', 'BTG', 'WULF', 'AAL', 'CIFR', 'PYPL', 'AVGO', 'CLSK', 'RIG', 'JOBY', 'GGB', 'ORCL', 'MU', 'BSX', 'ITUB', 'PATH', 'VZ', 'HBAN', 'QBTS', 'T', 'NOW', 'RGTI', 'SAN', 'UBER', 'RIOT', 'WMT', 'SOUN', 'DAY', 'PSLV', 'WBD', 'EOSE', 'ADT', 'CDE', 'QCOM', 'IONQ', 'APLD', 'COIN', 'BAC', 'LUMN', 'SNDK', 'B', 'RIVN', 'SIRI', 'CMCSA', 'NGD', 'CSCO', 'BMY', 'NOK', 'CRWV', 'ARM', 'SLB', 'HL', 'BTE', 'TTD', 'AG', 'CARR', 'RKT', 'AUR', 'SMR', 'HPE', 'HPQ', 'PTEN', 'U', 'RKLB', 'JBLU', 'MRK', 'CRM', 'USAR', 'FIG', 'PBR', 'FLNC', 'QS', 'VOD', 'FCX', 'CMG', 'BULL', 'KO', 'IBRX', 'EL', 'CORZ', 'UWMC', 'SHOP', 'TOST', 'APH', 'MRVL', 'RBLX', 'STLA', 'C', 'NBIS', 'CCL', 'XOM', 'LYFT', 'LYG', 'ASTS', 'KKR', 'UAA', 'DKNG', 'AGNC', 'CWAN', 'COTY', 'GLXY', 'WU', 'NCLH', 'CIG', 'META', 'PHYS', 'DOW', 'ARCC', 'ASX', 'PCG', 'TSM', 'RF', 'CSX', 'KGC', 'INFY', 'UUUU', 'ET', 'PINS', 'PCH', 'CRCL', 'KHC', 'CLF', 'CNH', 'MDLZ', 'NEM', 'EXK', 'KEY', 'AFRM', 'FAST', 'WFC', 'GSK', 'EQX', 'COMP', 'CCI', 'CX', 'ORLY', 'FITB', 'BAX', 'ABT', 'VLY', 'BX', 'BE', 'DIS', 'WIT', 'VG', 'TMC', 'MRNA', 'DVN', 'PAAS', 'PG', 'PANW', 'COHR', 'IOT', 'OBDC', 'CAG', 'MCHP', 'HUT', 'FSM', 'UNH', 'PL', 'ZETA', 'LUNR', 'ARES', 'SHEL', 'ENPH', 'HAL', 'FTNT', 'TPG', 'NKE', 'ON', 'AVTR', 'OKLO', 'BP', 'NOV', 'RAL', 'RELX', 'BABA', 'PPL', 'SBSW', 'UEC', 'WDC', 'TENB', 'HLN', 'BB', 'S', 'NEE', 'JD', 'KMI', 'PBR-A', 'RDDT', 'CNQ', 'RITM', 'BKR', 'VRNS', 'TEVA', 'TXN', 'MGM', 'GME', 'OXY', 'USB', 'EXC', 'CNC', 'CTRA', 'XYZ', 'IP', 'SONY', 'LRCX', 'CPRT', 'BCS', 'NVST', 'PR', 'MO', 'CVE', 'CVX', 'GLW', 'GEN', 'DELL', 'DOC', 'OS', 'AXL', 'ABBV', 'JNJ', 'AES', 'TEM', 'APP', 'PEP', 'ANET', 'NWSA', 'LCID', 'TME', 'CVS', 'SVM', 'IBKR', 'ERIC', 'XPEV', 'SM', 'AMCR', 'DT', 'MDT', 'CRDO', 'EQT', 'QXO', 'VTRS', 'NTNX', 'WMG', 'PSKY', 'UMC', 'AA', 'MBLY', 'BCE', 'COP', 'GPK', 'BBIO', 'XPO', 'FRSH', 'KMB', 'TRI', 'CZR', 'SCHW', 'WBS', 'KDP', 'TGB', 'SNOW', 'LITE', 'FISV', 'RBRK', 'CL', 'MET', 'JPM', 'NXE', 'GTM', 'SE', 'HST', 'MP', 'TFC', 'CMA', 'CPB', 'BTDR', 'RUN', 'FNB', 'SID', 'HMY', 'VICI', 'TEAM', 'NLY', 'EPD', 'YMM', 'GILD', 'GBDC', 'MS', 'V', 'RIO', 'DOCS', 'LUV', 'NDAQ', 'AMAT', 'OSCR', 'VFC', 'TPR', 'HBM', 'WMB', 'XEL', 'CTSH', 'MTCH', 'TU', 'VST', 'ROKU', 'IAG', 'DASH', 'CART', 'SPOT', 'BA', 'AGI', 'AEG', 'LLY', 'IVZ', 'PAYO', 'MPW', 'APA', 'FLG', 'CSGP', 'GM', 'EW', 'STM', 'O', 'CRGY', 'USAS', 'LBRT', 'TECK', 'GT', 'ADI', 'DDOG', 'COF', 'BN', 'BAM', 'MOS', 'FLEX', 'FOXA', 'FRMI', 'BEN', 'ELAN', 'LYB', 'VKTX', 'LI', 'DBRG', 'WDAY', 'CTVA', 'ALM', 'HUN', 'DJT', 'WRD', 'UAL', 'STX', 'EMR', 'STNE', 'SYM', 'CCJ', 'NET', 'VRT', 'GTLB', 'SEE', 'ETSY', 'NWG', 'KR', 'CRWD', 'CHWY', 'DX', 'GIS', 'EQNR', 'JCI', 'QGEN', 'ACI', 'DBX', 'ADBE', 'BBVA', 'UPS', 'ASAN', 'RPRX', 'INTU', 'PDD', 'PM', 'AMRZ', 'SBUX', 'M', 'TMUS', 'ACN', 'GAP', 'ELF', 'APO', 'EBAY', 'FE', 'FROG', 'CIVI', 'IBN']\n"
     ]
    }
   ],
   "source": [
    "# first step scrape yfinance for later ingesting with stock class.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def get_yfinance_tickers():\n",
    "    \"\"\"\n",
    "    input: None\n",
    "    output: returns tickers of most active stocks, stops collecting when yfinance runs out (1-3 loops)\n",
    "    \"\"\"\n",
    "    # list to hold data\n",
    "    tickers = []\n",
    "\n",
    "    # default starting index for 100 is ?start=0&count=100\n",
    "    start_ind = 0\n",
    "    count = 100\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    #header for request\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        #base URL (needs to change every loop)\n",
    "        url = f\"https://finance.yahoo.com/markets/stocks/most-active/?start={start_ind}&count={count}\"\n",
    "        # stop on error, call soup\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        # grab main container\n",
    "        container = soup.select_one(\"div.container.yf-1bczin\")\n",
    "\n",
    "        # if not main container stop\n",
    "        if not container:\n",
    "            break\n",
    "\n",
    "\n",
    "        # if no more data stop\n",
    "        if container.select_one(\"div.no-data\"):\n",
    "            break\n",
    "\n",
    "        links = soup.find_all(\"a\", {\"data-testid\": \"table-cell-ticker\"})\n",
    "\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get(\"href\")\n",
    "\n",
    "            # These are the link extensions to go to individual pages, \n",
    "            # in format /quote/SNAP/, which would be accessed like\n",
    "            # https://finance.yahoo.com/quote/MU/\n",
    "            # instead of drilling TWO levels deeper using BS4,\n",
    "            # which is a buggy and annoying process,\n",
    "            # the best practice I've found is to use RegEx.\n",
    "            #\n",
    "            # RegEx reduces time and memory complexity\n",
    "            # by skpping over redundant soup layering.\n",
    "            #\n",
    "            # I am opting to use regex to remove the chars:\n",
    "            #   /quote/ /\n",
    "\n",
    "            match = re.search(r\"/quote/([A-Z0-9.-]+)\", href)\n",
    "            if match:\n",
    "                cleaned_title = match.group(1)\n",
    "                tickers.append(cleaned_title)\n",
    "        time.sleep(2)\n",
    "\n",
    "        start_ind += 100\n",
    "\n",
    "    return tickers\n",
    "\n",
    "tickers = get_yfinance_tickers()\n",
    "print(tickers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aadeb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://finance.yahoo.com/quote/CADE/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_dic\n\u001b[0;32m---> 49\u001b[0m enriched_tickers \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_yfinance_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(enriched_tickers)\n",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m, in \u001b[0;36mscrape_yfinance_info\u001b[0;34m(list_of_tickers)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# stop on error, call soup\u001b[39;00m\n\u001b[1;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Find the container div\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Code/code1/.venv/lib/python3.9/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1023\u001b[0m     )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://finance.yahoo.com/quote/CADE/"
     ]
    }
   ],
   "source": [
    "# scraping yfinance info\n",
    "# base url https://finance.yahoo.com/quote/MU/\n",
    "\n",
    "# price will be gotten later with twelve api\n",
    "#\n",
    "\n",
    "def scrape_yfinance_info(list_of_tickers):\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    data_dic = {}\n",
    "    for ticker in list_of_tickers:\n",
    "\n",
    "        url = f\"https://finance.yahoo.com/quote/{ticker}/\"\n",
    "        # stop on error, call soup\n",
    "        response = requests.get(url, headers=headers)\n",
    "        try:\n",
    "            response.raise_for_status()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the container div\n",
    "        statistics_div = soup.find(\"div\", {\"data-testid\": \"quote-statistics\"})\n",
    "\n",
    "        if not statistics_div:\n",
    "            print(f\"No stats found for {ticker}\")\n",
    "            data_dic[ticker] = {}\n",
    "            continue\n",
    "\n",
    "\n",
    "        rows = statistics_div.find_all(\"li\")\n",
    "\n",
    "        data = {}\n",
    "        \n",
    "        for row in rows:\n",
    "\n",
    "            label = row.find(\"span\", class_=\"label\")\n",
    "            value = row.find(\"span\", class_=\"value\")\n",
    "\n",
    "            if label and value:\n",
    "                label_text = label.get_text(strip=True)\n",
    "                value_text = value.get_text(strip=True)\n",
    "\n",
    "                data[label_text] = value_text\n",
    "\n",
    "        data_dic[ticker] = data\n",
    "        time.sleep(5)\n",
    "\n",
    "    return data_dic\n",
    "\n",
    "enriched_tickers = scrape_yfinance_info(tickers)\n",
    "print(enriched_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ac719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next up one by one we are going to assemble a dictionary that has key's stocks, and google finance\n",
    "# info as a nested dictionary of information, e.g. {'F':{'q1_2025_profit':100000, 'market_cap':10000, etc.}}\n",
    "\n",
    "\n",
    "# going to still do google also but google requires knowing the index too which is annoying, but just means\n",
    "# I need to get this information from yfinance first.\n",
    "\n",
    "\n",
    "def get_google_info_w_yfin_tickers(list_of_tickers):\n",
    "    \"\"\"\n",
    "    input: List of tickers\n",
    "    output: returns dictionary of dictionary of tickers and info.\n",
    "    in the format of {'F':{'q1_2025_profit':100000, market_cap, etc.}}\n",
    "    \"\"\"\n",
    "    #header for request, in the future state this can be a class var\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    base_url = f'https://'\n",
    "    for i in list_of_tickers:\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
